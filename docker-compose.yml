services:
  model-fetcher:
    image: datamachines/git-lfs:latest
    environment:
      - SKIP_MODEL_FETCH=${SKIP_MODEL_FETCH:-false}
    volumes:
      - ./Kokoro-82M:/app/Kokoro-82M
    working_dir: /app/Kokoro-82M
    command: >
      sh -c "
        if [ \"$$SKIP_MODEL_FETCH\" = \"true\" ]; then
          echo 'Skipping model fetch...' && touch .cloned;
        else
          rm -f .git/index.lock;
          if [ -z \"$(ls -A .)\" ]; then
            git clone https://huggingface.co/hexgrad/Kokoro-82M .
            touch .cloned;
          else
            rm -f .git/index.lock && \
            git checkout main && \
            git pull origin main && \
            touch .cloned;
          fi;
        fi;
        tail -f /dev/null
      "
    healthcheck:
      test: ["CMD", "test", "-f", ".cloned"]
      interval: 5s
      timeout: 2s
      retries: 300
      start_period: 1s

  kokoro-tts:
    # image: ghcr.io/remsky/kokoro-fastapi:latest
    # Uncomment below to build from source instead of using the released image
    build:
      context: .
    volumes:
      - ./api/src:/app/api/src
      - ./Kokoro-82M:/app/Kokoro-82M
    ports:
      - "8880:8880"
    environment:
      - PYTHONPATH=/app:/app/Kokoro-82M
      # TTS Settings
      - MAX_CHUNK_SIZE=400 # Max of 500, best quality; Lower values improve latency
      - GAP_TRIM_MS=250 # Trim silence at the beginning and end of audio chunks
      # GPU Settings
      - CLEAR_CUDA_CACHE=true ### Clear CUDA cache after each request, and after warmup tests.
      - GPU_POOL_SIZE=0 ### Number of GPU model instances (0 = auto-scale based on memory)
      - GPU_MEMORY_PER_MODEL_GB=1.0 ### Memory per model in GB when auto-scaling
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      model-fetcher:
        condition: service_healthy

  # Gradio UI service [Comment out everything below if you don't need it]
  gradio-ui:
    # image: ghcr.io/remsky/kokoro-fastapi:latest-ui
    # Uncomment below to build from source instead of using the released image
    build:
      context: ./ui
    ports:
      - "7860:7860"
    volumes:
      - ./ui/data:/app/ui/data
      - ./ui/app.py:/app/app.py  # Mount app.py for hot reload
    environment:
      - GRADIO_WATCH=True  # Enable hot reloading
